# 딥러닝 구조 / 모델 정리

## 목차

- [RNN의 구조와 한계](./RNN.md)
- [LSTM vs GRU 차이](./LSTM_vs_GRU.md)
- [Attention 메커니즘](./Attention.md)
- [Transformer 구조](./Transformer.md)
- [Positional Encoding의 필요성](./Positional_Encoding.md)
- [Self-Attention 원리](./Self_Attention.md)
- [BERT vs GPT 구조 비교](./BERT_vs_GPT.md)
- [Encoder-only, Decoder-only, Encoder-Decoder 구조 비교](./Transformer_Architecture.md)
- [Multi-head Attention](./Attention.md#multi-head-attention)
- [Residual Connection의 필요성과 효과](./Residual_Connection.md)
- [Layer Normalization vs Batch Normalization](./Normalization_Techniques.md)
- [ViT (Vision Transformer)](./ViT.md)
- [U-Net 이미지 분할 구조](./UNet.md)
- [Autoencoder의 구성과 역할](./Autoencoder.md)
- [VAE (Variational Autoencoder)](./Autoencoder.md#vae)
- [GAN 구조 (Generator & Discriminator)](./GAN.md)
