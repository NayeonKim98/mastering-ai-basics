# 목차

- ### K-NN
- ### Decision Tree
- ### SVM
- ### K-Means

<br>


<br>


<br>

# SVM (Support Vector Machine)

<br>

## 핵심 개념

- 데이터를 분류하기 위해 가장 margin이 넓은 경계선을 찾는 알고리즘
- 단순한 분류기이지만, 비선형 문제도 `커널 기법`으로 해결 가능

    - ### 커널 트릭 (Kernel Trick)
        
        - 데이터를 선형 분리 불가능한 경우
        - 고차원 공간으로 매핑해서 선형 분리를 가능하게 함
        - 데이터가 평면(2D)에 있을 때는 선형으로 나누기 어려울 수 있는데, 커널을 3차원으로 만드는 것. 
        - 비유 : 칠판의 고무자석. 고무판을 우그러뜨리면 처음에 뒤섞여있던 빨간색/파란색 자석들이 서로 위아래로 떨어져 직선 하나로 나눌 수 있게 되는 상황과 비슷
        <br>

        | 커널 종류 | 설명 |
        | --- | --- |
        | 선형 커널 | 일반적인 직선 기준 |
        | RBF(가우시안) 커널 | 데이터가 동그랗게 뭉친 경우 좋음 |
        | 다항식 커널 | 비선형 경계가 필요한 경우 |

        <br>
        - 예시 코드:

        ```python
        from sklearn.svm import SVC

        # 데이터: (점수, 수면시간), 레이블은 '합격/불합격'
        X = [[70, 6], [80, 7], [90, 6], [40, 8], [50, 7]]
        y = ['합격', '합격', '합격', '불합격', '불합격']

        # 모델: 커널은 선형(linear)
        model = SVC(kernel='linear')
        model.fit(X, y)

        print(model.predict([[65, 6]]))  # 출력: ['합격'] or ['불합격']
        ```

- 비유 : 학생들을 줄세우는데, 각 반의 가장 키 큰 학생들 사이에 선을 긋는 방식. 즉, **두 집단의 사이 간격을 최대한 넓히는 선(결정 경계)를 그린다!**

<br>

## 작동 원리

1. 데이터를 분류하기 위해 결정 경계(Hyperplane)를 찾음
2. 가장 가까운 데이터(Support Vectors)와의 거리(Margin)를 최대화
3. 선형 분리가 어려울 경우, 커널 함수를 사용해 고차원 공간으로 변환

### 과정 정리

1. 초평면 정의
    - 2차원 : 선
    - 3차원 : 평면
    - n차원 : 초평면

2. 서포트 벡터 찾기
    - 경계와 가장 가까운 점들(모델 결정에 핵심 역할)

3. 마진 최대화
    - 두 클래스 사이의 가장 큰 여유 공간 확보

<br>

## 수학적 표현

- 결정 경계는 다음과 같은 선형 함수로 표현
    - f(x) = w^Tx + b = 0
    
    - w : 가중치 벡터 (선의 기울기)
    - b : 절편(bias)
    - | w^Tx + b |가 클수록 분류의 확신도가 높음

    - 최적화 문제로 바뀜. margin을 최대화하면서 제약조건을 만족하는 w, b를 찾는 문제.
        - 마진을 최대화 = |w|를 최소화

<br>

## SVM의 장단점

| 장점 | 단점 |
| --- | --- |
| 고차원 데이터에서도 효과적 | 느림(특히 데이터 양 많을 때) |
| 마진 기반으로 일반화 성능 좋음 | 파라미터 튜닝 필요 (C, 커널 등) |
| 다양한 커널로 비선형 문제 해결 가능 | 해석이 어려움(트리처럼 시각화 힘듦) |

<br>

## 관련 개념

| 용어 | 설명 |
| --- | --- |
| 마진(Margin) | 결정 경계와 가장 가까운 데이터 사이의 거리 |
| 서포트 벡터 | 마진에 닿아 있는 데이터 포인트 |
| 커널 함수 | 고차원 공간으로 데이터를 옮기는 방법 |
| C값 | 오차 허용 정도(오차에 얼마나 민감할지) -> 작으면 마진 중시, 크면 오차 최소화 중시 |

<br>

# K-Means Clustering (K-평균 군집화)

<br>

## 핵심 개념

- 레이블이 없는 테이터를 유사한 특징끼리 묶는 비지도 학습 알고리즘
- K개의 중심점(centroid)를 기준으로 데이터를 나누고, 반복적으로 군집(cluster)을 개선
- 비유 : 학생들의 키와 체중을 기준으로 기준점을 만들고, 비슷한 친구들끼리 묶기

<br>

## 작동 원리 (반복 최적화 과정)

1. 원하는 군집 개수 K를 정함
2. 랜덤하게 K개의 중심점(centroid)을 초기화
3. 각 데이터를 가장 가까운 중심점에 할당
4. 각 군집의 중심점을 데이터 평균으로 업데이트
5. 중심점이 더 이상 변하지 않을 때까지 3~4번 반복

<br>

## 수학적 표현

- 목적 : 각 데이터와 중심점 간의 거리의 제곱합을 최소화

![K-NN_exp](K-NN_exp.png)

- Ci : i 번째 군집
- 𝜇i : i 번째 군집의 중심점
- ||x - 𝜇i||^2 : 거리의 제곱 (보통 유클리디안 거리 사용)

<br>

## 예시 코드

```python
from sklearn.cluster import KMeans
import numpy as np

# 예시 데이터 (공부시간, 수면시간)
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])

# K=2로 군집화
model = KMeans(n_clusters=2, random_state=42)
model.fit(X)

print("라벨:", model.labels_)
print("중심점:", model.cluster_centers_)
```

<br>

## K-Mean의 장단점

| 장점 | 단점 |
| --- | --- |
| 구현이 간단, 계산 속도가 빠르고 효율적 | K값(클러스터 수)를 미리 알아야 함 -> 도메인 지식이 없으면 최적 K 결정 어려움 |
| 대규모 데이터일수록 계산 기반의 성능이 좋아짐 | 초기 중심값에 민감해 초기값에 따라 결과가 달라질 수 있음(`로컬 최적해` 가능성) |
| 시각화 및 해석이 쉬움 | 비구형(비선형) 데이터에 한계 -> 둥근 형태가 아닌 클러스터는 잘 구분 못함 |
| 중심을 반복 갱신하는 방식으로 비교적 빨리 수렴함 | 스케일(크기)에 민감 -> 거리 기반으로 정규화가 필수 |
| 간단한 계산으로 메모리 자원 소모 적음 | 이상치에 취약 -> 평균 기반이므로 극단값에 영향을 많이 받음 |
| 거리 기반 | 유클리드 거리 기반으로 직관적 | 군집 간 밀도 차이 고려 불가 -> 밀도나 분포가 다르면 부정확한 군집화 가능 |

### 로컬 최적해(local optimum)

- 전체 중에 최고는 아니지만 주변에서는 최고인 답을 찾게 될 때

<br>

## 관련 개념

| 개념 | 설명 |
| --- | --- |
| 중심점 | 각 군집의 중심(평균값) |
| 군집 수 K | 사전에 정해야 하며, 잘못 설정하면 성능 저하 |
| 정규화 | 변수 간 단위가 다를 경우 필요 |
| `엘보우 기법` | 적절한 K값을 찾기 위한 시각적 방법 |
| `실루엣 계수` | 군집의 품질을 평가하는 지표 (0~1 사이) |

#### 엘보우 기법(Elbow Method)

- 적절한 K값(군집 수)를 고르기 위한 시각적 도구
- K가 커질수록 당연히 군집 내부의 오차(SSE, WCSS)가 줄어듦
- 하지만 어느 순간부터는 덜 줄어들고, 굳이 늘릴 필요 없는 시점이 나옴.
- 팔꿈치 꺾이는 지점처럼 생김 => 적절한 K값인 지점

#### 실루엣 계수

- 군집을 얼마나 잘 만들었는지 정량적으로 평가하는 방법
- 값은 0~1 사이이고, 1에 가까울수록 좋은 군집화
- 계산 방법 : silhouette = (b - a)/max(a, b)
    - a : 자기 클러스트 안에서 평균 거리(같은 군집에서 얼마나 떨어져 있는가)
    - b : 다른 클러스터들과의 거리 중 가장 작은 값(다른 군집과 얼마나 떨어져 있는가)
- 나랑 같은 그룹끼리는 가까워야하고,
- 나랑 다른 그룹은 멀어야 함.
