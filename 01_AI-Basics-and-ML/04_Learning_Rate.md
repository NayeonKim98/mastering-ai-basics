## 학습률(Learning Rate)

- 정의 : 딥러닝에서 모델이 얼마나 빠르게 or 천천히 학습할지 결정하는 하이퍼파라미터(hyperparameter)

- 기호는 보통 η(에타) or α(알파)로 표기
    - `경사 하강법 업데이트 공식` θ = θ - η⋅∇J(θ)

        - θ : 모델의 파라미터 (가중치, weight)
        - J(θ) : 손실함수(loss function)
        - ∇J(θ) : 기울기(gradient)

- 경사 하강법에서 **매 스텝마다 파라미터를 얼마나 움직일지** 결정하는 크기

### 학습률이 중요한 이유
1. #### 최적화 속도와 안정성에 직접적 영향
    - 학습률이 너무 크면, 모델이 최소값을 지나치거나 발산해 학습이 불안정. 반대로 학습률이 너무 작으면 시간이 오래 소요되거나 지역 최소값에 머물러 전체적 성능 저하.

2. #### 모델 성능에 결정적 역할
    - 적절한 학습률은 **모델이 더 좋은 성능**을 낼 수 있게 함.

3. #### 손실 함수 최소화 과정의 핵심
    - 경사 하강법과 같은 최적화 알고리즘에서 모델이 손실 함수를 얼마나 빠르게, 얼마나 안정적으로 최소화할 수 있는지에 직접적 영향

4. #### 실제 모델 개발 및 운영에 미치는 영향
    - 학습률을 잘 조정하면 적은 데이터로도 효율적인 학습이 가능. **전체 개발 주기를 단축하며 비용 절감 효과**

### 주의할 점
- 산을 내려가는 상황에 비유해보자.
    - 손실 함수 : 울퉁불퉁한 산의 표면
    - 기울기 : 아래방향을 가르키는 이정표
    - 학습률 : 한 걸음의 크기

- ### 보폭이 너무 크면?
    - 산을 벗어나거나, 계속 튀어 수렴하지 않음
    - 수학적으로는 `overshooting`이 일어남
        #### overshooting
        - 모델이 최소값으로 수렴하지 않고 **목표 지점이나 목표 방향향을 지나쳐버리는 현상**

- ### 보폭이 너무 작으면?
    - 수렴은 하지만 시간이 너무 오래 걸리거나 로컬 미니마(local minima) 근처에서 멈춰버림

### 그래서 어떻게 설정하는가?
1. #### 경험적 튜닝 (Trial-and-error)
    - 0.1, 0.01, 0.001 같은 값부터 실험

2. #### 학습률 감소 전략 (Learning Rate Scheduling)
    - epoch이 늘어날수록 learning rate 점점 줄임. 왜 줄이느냐? 초반에 빠르게 학습하다 후반에 미세조정
    - ReduceLROnPlateau(모델 성능이 멈췄을 때 학습률을 줄이자), StepLR(몇 epoch마다 일정하게 줄이자), CosineAnnealing(서서히 줄이다 마지막에 확 떨어뜨리자) 등. 대표적인 전략들. 

3. #### 적응형 학습률 (Adaptive LR)
    - Adam, RMSProp, Adagrad 등은 학습률을 자동으로 조정해줌
