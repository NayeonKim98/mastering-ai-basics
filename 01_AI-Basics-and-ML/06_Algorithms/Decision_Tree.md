# 결정 트리 (Decision Tree)

<br>

## 핵심 개념

- 데이터를 조건에 따라 가지를 나누는 트리 구조를 사용하여 예측
- 질문을 통해 데이터를 나누면서 분류하거나 회귀
- #### 비유 : 스무고개 게임
    - "동물인가요?" -> "네 발인가요?" -> ...

<br>

## 작동 원리

- 특정 기준(feature, 임계값)에 따라 데이터를 나눔
- 각 분기점(node)가 질문
- 각 가지(branch)는 질문에 대한 대답(Yes/No)
- 최종 리프 노드는 예측값(분류 or 숫자)

### 과정 정리

1. 모든 데이터를 가져와서
2. 가장 데이터를 잘 나눌 수 있는 질문을 고름
    -> 이때 `불순도(impurity)`를 기준으로 판단 (예: Gini, Entropy)
    
    - #### 불순도(impurity)

        - 그 질문이 얼마나 좋은 질문인지 판단하는 기준
        - 이 질문이 데이터를 얼마나 한쪽으로 깔끔하게 나누는가를 수치로 나타낸 것
        - 예를 들어, 나이가 35세 이하인가요? -> Yes그룹(10세(애니), 12세(애니), 35세(드라마)/No(40세(드라마)) => **Yes그룹에 애니와 드라마가 섞여있으니 불순도가 높음**

        | 불순도 측정 | 수식 | 특징 |
        | --- | --- | --- |
        | 지니 지수(Gini Index) | 1 - ∑pi^2 (pi=클래스비율(예: 애니 = 2/3)) | 빠르고 효율적, 많이 사용 |
        | 엔트로피(Entropy) | - ∑pilog2pi | 정보이론 기반, 정보 이득 계산 |

3. 데이터를 둘로 나눔
4. 나눠진 두 그룹에 대해 다시 같은 작업 반복
5. 멈추는 조건 도달 시 예측값을 출력

<br>

## 예시

```python
from sklearn.tree import DecisionTreeClassifier

# 데이터: [나이, 통화시간], 레이블은 '구매 여부'
X = [[25, 5], [30, 10], [45, 1], [35, 2]]
y = ['구매', '구매', '비구매', '비구매']

model = DecisionTreeClassifier(criterion='gini')
model.fit(X, y)

print(model.predict([[40, 3]]))  # 예측 결과
```

<br>

## 장단점

| 항목 | 장점 | 단점 |
| --- | --- | --- |
| 해석 용이 | 트리 구조가 사람이 이해하기 쉬움 | 과적합(overfiting)되기 쉬움(데이터를 매우 구체적으로 나누려하기 때문에) |
| 데이터 전처리 | 정규화 필요 없음, 스케일 영향이 적음(특성 값이 크든 작든 상관없이 잘 작동) | 작은 변화에도 트리 구조가 확 바뀜(분기점을 기준으로 왼쪽그룹이 될 수도, 오른쪽그룹이 될 수도) |
| 속도 | 학습/예측 빠름 | 깊은 트리는 느려지고 복잡해질 수 있음

<br>

## 관련 개념

| 용어 | 설명 |
| --- | --- |
| 가지치기(Pruning) | 너무 깊은 트리를 잘라서 과적합 방지 |
| 앙상블(Ensemble) | 여러 트리를 모아 성능을 향상(예: Random Forest) |
| 불순도(Impurity) | 노드가 얼마나 섞여 있는지를 나타내는 지표(Gini, Entropy 등) |